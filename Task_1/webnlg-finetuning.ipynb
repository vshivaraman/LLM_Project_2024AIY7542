{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9531558,"sourceType":"datasetVersion","datasetId":5804774}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp -r /kaggle/input/webnlg-parallel .","metadata":{"execution":{"iopub.status.busy":"2024-11-25T02:29:22.835204Z","iopub.execute_input":"2024-11-25T02:29:22.835788Z","iopub.status.idle":"2024-11-25T02:29:23.961318Z","shell.execute_reply.started":"2024-11-25T02:29:22.835755Z","shell.execute_reply":"2024-11-25T02:29:23.960184Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"%cd webnlg-parallel/","metadata":{"execution":{"iopub.status.busy":"2024-11-25T02:29:25.246318Z","iopub.execute_input":"2024-11-25T02:29:25.246713Z","iopub.status.idle":"2024-11-25T02:29:25.252319Z","shell.execute_reply.started":"2024-11-25T02:29:25.246680Z","shell.execute_reply":"2024-11-25T02:29:25.251427Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/webnlg-parallel/webnlg-parallel\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-11-25T02:29:27.979812Z","iopub.execute_input":"2024-11-25T02:29:27.980469Z","iopub.status.idle":"2024-11-25T02:29:29.030444Z","shell.execute_reply.started":"2024-11-25T02:29:27.980435Z","shell.execute_reply":"2024-11-25T02:29:29.029406Z"},"trusted":true},"outputs":[{"name":"stdout","text":"multigpu.py  webnlg\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile multigpu.py\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n# from datautils import MyTrainDataset\n\nimport torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nimport os\n\n\nfrom transformers import BartTokenizer, BartForConditionalGeneration, AdamW, get_scheduler, MBartTokenizer\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nfrom tqdm import tqdm\n\n\ndef ddp_setup(rank, world_size):\n    \"\"\"\n    Args:\n        rank: Unique identifier of each process\n        world_size: Total number of processes\n    \"\"\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n    torch.cuda.set_device(rank)\n    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n\nclass WebNLGDataset(Dataset):\n    def __init__(self, dset_path, dsetType: str, tokenizer):\n        if dsetType == 'train':\n            files = ['train.source', 'train.target']\n        elif dsetType == 'val':\n            files = ['val.source', 'val.target']\n        else:\n            files = ['test_both.source', 'test_both.target']\n        \n        with open(dset_path + files[0], 'r') as f:\n            source_raw = f.readlines()\n\n        with open(dset_path + files[1], 'r') as f:\n            target_raw = f.readlines()\n\n        self.source_tok = tokenizer(source_raw, truncation=True, padding='max_length', max_length=384, return_tensors = 'pt')\n        self.target_tok = tokenizer(target_raw, truncation=True, padding='max_length', max_length=384, return_tensors = 'pt')\n        \n    def __len__(self): # returns the total number of samples in the dataset\n        assert self.source_tok['input_ids'].shape[0] == self.target_tok['input_ids'].shape[0]\n        return self.source_tok['input_ids'].shape[0]\n        \n    def __getitem__(self, idx):\n        return {'input_ids': self.source_tok['input_ids'][idx], \n                'attention_mask': self.source_tok['attention_mask'][idx],\n                'labels': self.target_tok['input_ids'][idx],            \n               }\n\nclass Trainer:\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        train_data: DataLoader,\n        val_data: DataLoader,\n        tokenizer: BartTokenizer,\n        optimizer: torch.optim.Optimizer,\n        gpu_id: int,\n        save_every: int,\n    ) -> None:\n        self.gpu_id = gpu_id\n        self.model = model.to(gpu_id)\n        self.train_data = train_data\n        self.val_data = val_data\n        self.tokenizer = tokenizer\n        self.optimizer = optimizer\n        self.save_every = save_every\n        self.model = DDP(model, device_ids=[gpu_id])\n\n    def _run_batch(self, input_ids, attention_mask, labels, run_type='train', eval_loss=None):\n        if run_type == 'train':\n            self.optimizer.zero_grad()\n            # print(input_ids)\n            # print(labels)\n            # print(input_ids.shape, attention_mask.shape, labels.shape)\n            output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            # print(output)\n            # loss = F.cross_entropy(output, targets)\n            loss = output.loss\n            # print(loss)\n            ### TODO: Add lr_scheduler ###\n            loss.backward()\n\n            self.optimizer.step()\n            return output, eval_loss\n        elif run_type == 'validate':\n            with torch.no_grad():\n                output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                eval_loss += output.loss.item()\n            return output, eval_loss\n\n    def _run_epoch(self, epoch, epoch_type='train'):\n\n        if epoch_type == 'train':\n            print(\"------ Training! ------\")\n            data = self.train_data\n            eval_loss = None\n\n        elif epoch_type == 'validate':\n            print(\"------ Validating! ------\")\n            data = self.val_data\n            eval_loss = 0\n\n        # b_sz = len(next(iter(self.train_data))[0])\n        # b_sz = len(next(iter(data))[0])\n        # print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Steps: {len(data)}\")\n        data.sampler.set_epoch(epoch)\n        # for source, targets in self.train_data:\n        #     source = source.to(self.gpu_id)\n        #     targets = targets.to(self.gpu_id)\n        #     self._run_batch(source, targets)\n        for batch_idx, batch in tqdm(enumerate(data), total=len(data)):\n            input_ids = batch['input_ids'].to(self.gpu_id)\n            attention_mask = batch['attention_mask'].to(self.gpu_id)\n            labels = batch['labels'].to(self.gpu_id)\n\n            if epoch_type == 'train':\n                _, __ = self._run_batch(input_ids=input_ids, attention_mask=attention_mask, labels=labels, eval_loss=eval_loss, run_type='train')\n            elif epoch_type == 'validate':\n                _, eval_loss = self._run_batch(input_ids=input_ids, attention_mask=attention_mask, labels=labels, eval_loss=eval_loss, run_type='validate')\n        if epoch_type == 'validate':\n            print(f\"Epoch {epoch+1}: Evaluation Loss = {eval_loss / len(self.val_data)}\")\n\n    def _save_checkpoint(self, epoch):\n        ckp = self.model.module.state_dict()\n        PATH = \".\"\n        #         torch.save(ckp, PATH)\n        self.model.module.save_pretrained(PATH)\n        self.tokenizer.save_pretrained(PATH)\n\n        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n\n    def train(self, max_epochs: int):\n        for epoch in range(max_epochs):\n            self._run_epoch(epoch=epoch, epoch_type='train')\n            self._run_epoch(epoch=epoch, epoch_type='validate')\n            if self.gpu_id == 0 and (epoch + 1) % self.save_every == 0:\n                self._save_checkpoint(epoch)\n                \n    def generate_predictions(self, texts):\n        model = self.model.module\n        model.to('cuda')\n        model.eval()\n        predictions = []\n    \n        for text in tqdm(texts):\n            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n            with torch.no_grad():\n                output_sequences = model.generate(**inputs.to('cuda'), num_beams=3, max_new_tokens=300, pad_token_id=model.config.eos_token_id)\n        \n            decoded_preds = self.tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n            predictions.extend(decoded_preds)\n    \n        return predictions \n\ndef load_train_objs(preTmodel):\n    # train_set = MyTrainDataset(2048)  # load your dataset\n    #     model_path = './model_cache/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177'\n    if preTmodel == 'bart':\n        tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n        model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n    elif preTmodel == 'gpt2':\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS (since GPT-2 does not have a dedicated pad token)\n        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n    elif preTmodel == 'T5':\n        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")  # Using t5-base instead of t5-small\n        model = T5ForConditionalGeneration.from_pretrained('t5-small')\n        # Ensure all necessary special tokens are present\n        special_tokens = {\n            'pad_token': '[PAD]',\n            'eos_token': '</s>',\n            'bos_token': '<s>',\n        }\n        tokenizer.add_special_tokens(special_tokens)\n        model.resize_token_embeddings(len(tokenizer))\n    new_tokens = ['<H>', '<R>', '<T>']\n    tokenizer.add_special_tokens({'additional_special_tokens': new_tokens})\n    model.resize_token_embeddings(len(tokenizer))\n    dataset_path = './webnlg/'\n    train_dataset = WebNLGDataset(dataset_path, 'train', tokenizer)\n    val_dataset = WebNLGDataset(dataset_path, 'val', tokenizer)\n    # model = torch.nn.Linear(20, 1)  # load your model\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n    return train_dataset, val_dataset, model, tokenizer, optimizer\n\n\ndef prepare_dataloader(dataset: Dataset, batch_size: int):\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        pin_memory=True,\n        shuffle=False,\n        sampler=DistributedSampler(dataset)\n    )\n\ndef main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):\n    ddp_setup(rank, world_size)\n    train_dataset, val_dataset, model, tokenizer, optimizer = load_train_objs(\"T5\")\n    train_data = prepare_dataloader(train_dataset, batch_size)\n    val_data = prepare_dataloader(val_dataset, batch_size)\n    trainer = Trainer(model, train_data, val_data, tokenizer, optimizer, rank, save_every)\n    trainer.train(total_epochs)\n    with open('./webnlg/'+ 'test_both.source', 'r') as f:\n        test_source = f.readlines()\n    with open('./webnlg/' + 'test_both.target', 'r') as f:\n        test_target = f.readlines()\n\n    print('Calculating BLEU Score')\n    predictions = trainer.generate_predictions(test_source)\n    from nltk.translate.bleu_score import corpus_bleu\n    references = [[ref.split()] for ref in test_target]  # Reference texts should be a list of lists of tokens\n    predicted_tokens = [pred.split() for pred in predictions]  # Predictions should also be a list of lists of tokens\n    bleu_score = corpus_bleu(references, predicted_tokens)\n    print(f\"BLEU score: {bleu_score:.4f}\")\n    destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description='simple distributed training job')\n    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n    parser.add_argument('--batch_size', default=12, type=int, help='Input batch size on each device (default: 32)')\n    args = parser.parse_args()\n\n    world_size = torch.cuda.device_count()\n    mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T02:37:57.763350Z","iopub.execute_input":"2024-11-25T02:37:57.763682Z","iopub.status.idle":"2024-11-25T02:37:57.773200Z","shell.execute_reply.started":"2024-11-25T02:37:57.763652Z","shell.execute_reply":"2024-11-25T02:37:57.772375Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Overwriting multigpu.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!python multigpu.py 3 2","metadata":{"execution":{"iopub.status.busy":"2024-11-25T02:58:31.939181Z","iopub.execute_input":"2024-11-25T02:58:31.939543Z","iopub.status.idle":"2024-11-25T03:37:15.274982Z","shell.execute_reply.started":"2024-11-25T02:58:31.939510Z","shell.execute_reply":"2024-11-25T03:37:15.273865Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[W1125 02:58:38.646370313 socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:12355 (errno: 99 - Cannot assign requested address).\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n------ Training! ------\n[GPU1] Epoch 0 | Steps: 755\n------ Training! ------\n[GPU0] Epoch 0 | Steps: 755\n100%|█████████████████████████████████████████| 755/755 [08:44<00:00,  1.44it/s]\n------ Validating! ------\n[GPU0] Epoch 0 | Steps: 37\n100%|█████████████████████████████████████████| 755/755 [08:44<00:00,  1.44it/s]\n------ Validating! ------\n[GPU1] Epoch 0 | Steps: 37\n100%|███████████████████████████████████████████| 37/37 [00:08<00:00,  4.26it/s]\nEpoch 1: Evaluation Loss = 0.15449468650527903\n------ Training! ------\n[GPU1] Epoch 1 | Steps: 755\n100%|███████████████████████████████████████████| 37/37 [00:08<00:00,  4.14it/s]\nEpoch 1: Evaluation Loss = 0.15766625831256043\n------ Training! ------\n[GPU0] Epoch 1 | Steps: 755\n100%|█████████████████████████████████████████| 755/755 [08:46<00:00,  1.43it/s]\n------ Validating! ------\n[GPU1] Epoch 1 | Steps: 37\n100%|█████████████████████████████████████████| 755/755 [08:46<00:00,  1.44it/s]\n------ Validating! ------\n[GPU0] Epoch 1 | Steps: 37\n100%|███████████████████████████████████████████| 37/37 [00:08<00:00,  4.25it/s]\nEpoch 2: Evaluation Loss = 0.08548669698270592\n------ Training! ------\n[GPU1] Epoch 2 | Steps: 755\n100%|███████████████████████████████████████████| 37/37 [00:08<00:00,  4.15it/s]\nEpoch 2: Evaluation Loss = 0.08566765517399118\n  0%|                                           | 1/755 [00:00<05:45,  2.18it/s]Epoch 1 | Training checkpoint saved at .\n------ Training! ------\n[GPU0] Epoch 2 | Steps: 755\n100%|█████████████████████████████████████████| 755/755 [08:46<00:00,  1.43it/s]\n------ Validating! ------\n[GPU1] Epoch 2 | Steps: 37\n100%|█████████████████████████████████████████| 755/755 [08:45<00:00,  1.44it/s]\n------ Validating! ------\n[GPU0] Epoch 2 | Steps: 37\n100%|███████████████████████████████████████████| 37/37 [00:08<00:00,  4.24it/s]\nEpoch 3: Evaluation Loss = 0.07121734089545302\nCalculating BLEU Score\n100%|███████████████████████████████████████████| 37/37 [00:08<00:00,  4.14it/s]\nEpoch 3: Evaluation Loss = 0.06958997632200653\nCalculating BLEU Score\n100%|███████████████████████████████████████| 1862/1862 [11:33<00:00,  2.69it/s]\n100%|██████████████████████████████████████▉| 1860/1862 [11:34<00:01,  1.94it/s]BLEU score: 0.2582\n100%|███████████████████████████████████████| 1862/1862 [11:35<00:00,  2.68it/s]\nBLEU score: 0.2582\n","output_type":"stream"}],"execution_count":14}]}