{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9853674,"sourceType":"datasetVersion","datasetId":6046641},{"sourceId":9968789,"sourceType":"datasetVersion","datasetId":6132654},{"sourceId":9999334,"sourceType":"datasetVersion","datasetId":6154675}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp -r /kaggle/input/llm-qald-9 .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:55:38.929509Z","iopub.execute_input":"2024-11-25T01:55:38.929827Z","iopub.status.idle":"2024-11-25T01:55:39.963086Z","shell.execute_reply.started":"2024-11-25T01:55:38.929802Z","shell.execute_reply":"2024-11-25T01:55:39.961927Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%cd /kaggle/working/llm-qald-9","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:55:39.964401Z","iopub.execute_input":"2024-11-25T01:55:39.964694Z","iopub.status.idle":"2024-11-25T01:55:39.971616Z","shell.execute_reply.started":"2024-11-25T01:55:39.964665Z","shell.execute_reply":"2024-11-25T01:55:39.970672Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/llm-qald-9\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile run.py\n# Read the data first\nimport re\nimport json\n\ndef preprocess_amr_data(data):\n    amr_complete = []\n    text_complete = []\n\n    for entry in data.keys():\n        amr_graph = data[entry]['amr']\n        target_text = data[entry]['text']\n        model_input = amr_graph.strip()\n        model_output = target_text.strip()\n        \n        amr_complete.append(model_input)\n        text_complete.append(model_output)\n\n    return amr_complete, text_complete\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nimport os\n\nfrom transformers import BartTokenizer, BartForConditionalGeneration, AdamW, get_scheduler, MBartTokenizer\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom tqdm import tqdm\n\n\n# Load AMR data from a file\nwith open('qald-9-amr-test.txt', 'r', encoding='utf-8') as f:\n    amr_data_test = f.read()\nwith open('qald-9-amr-train.txt','r',encoding='utf-8') as f:\n    amr_data_train=f.read()\n\n# with open('massive_amr_test.txt', 'r', encoding='utf-8') as f:\n#     amr_data_test = f.read()\n# with open('massive_amr.txt','r',encoding='utf-8') as f:\n#     amr_data_train=f.read()\n\nimport re\ndef split_amr_strings(amr_text):\n    amr_pattern = r\"(# ::snt .+?)(?=\\n# ::snt|\\Z)\"\n    amr_blocks = re.findall(amr_pattern, amr_text, flags=re.DOTALL)\n    return amr_blocks\n\ndef create_amr_dict(amr_text):\n    amr_strings = split_amr_strings(amr_text)\n    amr_dict = {}\n    amr_list = []\n    text_list = []\n    for idx, amr_string in enumerate(amr_strings):\n        # Split each block into the sentence part and the AMR part\n        snt_part = amr_string.split('\\n', 1)[0].strip()  # First line is the sentence part\n        text_part = re.sub('# ::snt ','',snt_part)\n        amr_part = amr_string[len(snt_part):].strip()  # Rest is the AMR part\n        \n        # Add to dictionary\n        amr_dict[idx] = {\n            \"amr\": amr_part,\n            \"text\": text_part\n        }\n        amr_list.append(amr_part)\n        text_list.append(text_part)\n    \n    return amr_dict, amr_list, text_list\n\namr_data_train, _, __ = create_amr_dict(amr_data_train)\namr_data_test, amr_test, text_test = create_amr_dict(amr_data_test)\n\n# Read the data first\nimport re\nimport json\n\ndef preprocess_amr_data(data):\n    amr_complete = []\n    text_complete = []\n\n    for entry in data.keys():\n        amr_graph = data[entry]['amr']\n        target_text = data[entry]['text']\n        model_input = amr_graph.strip()\n        model_output = target_text.strip()\n        \n        amr_complete.append(model_input)\n        text_complete.append(model_output)\n\n    return amr_complete, text_complete\n\nclass QALD_9_AMRDataset(Dataset):\n    def __init__(self, data, tokenizer):\n     \n        #train_amr, train_text = preprocess_amr_data(train)\n        amr, text = preprocess_amr_data(data)\n        self.amr_len = len(amr)\n        self.text_len = len(text)\n        prefixed_amr = [\"translate AMR to text: \" + x for x in amr]\n\n        self.amr = tokenizer(amr, truncation=True, padding='max_length', max_length=384, return_tensors = 'pt')\n        #print(self.amr)\n        self.text = tokenizer(text, truncation=True, padding='max_length', max_length=384, return_tensors = 'pt')\n  \n    def __len__(self): # returns the total number of samples in the dataset\n        assert self.amr_len == self.text_len\n        return self.amr_len\n        \n    def __getitem__(self, idx):\n        return {'amr': self.amr['input_ids'][idx], \n                'att_mask': self.amr['attention_mask'][idx],\n                'text': self.text['input_ids'][idx]\n               }\n\ndef ddp_setup(rank, world_size):\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n    torch.cuda.set_device(rank)\n    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n    \nclass Trainer:\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        train_data: DataLoader,\n        val_data: DataLoader,\n        tokenizer: BartTokenizer,\n        optimizer: torch.optim.Optimizer,\n        gpu_id: int,\n        save_every: int,\n    ) -> None:\n        self.gpu_id = gpu_id\n        self.model = model.to(gpu_id)\n        self.train_data = train_data\n        self.val_data = val_data\n        self.tokenizer = tokenizer\n        self.optimizer = optimizer\n        self.save_every = save_every\n        self.model = DDP(model, device_ids=[gpu_id])\n\n    def _run_batch(self, input_ids, attention_mask, labels, run_type='train', eval_loss=None):\n        if run_type == 'train':\n            self.optimizer.zero_grad()\n            output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = output.loss\n            loss.backward()\n\n            self.optimizer.step()\n            return output, eval_loss\n        elif run_type == 'validate':\n            with torch.no_grad():\n                output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                eval_loss += output.loss.item()\n            return output, eval_loss\n\n    def _run_epoch(self, epoch, epoch_type='train'):\n        \n        if epoch_type == 'train':\n            print(\"------ Training! ------\")\n            data = self.train_data\n            eval_loss = None\n            print(type(self.train_data), len(self.train_data))\n\n        elif epoch_type == 'validate':\n            print(\"------ Validating! ------\")\n            data = self.val_data\n            eval_loss = 0\n\n        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Steps: {len(data)}\")\n        data.sampler.set_epoch(epoch)\n\n        for batch_idx, batch in tqdm(enumerate(data), total=len(data)):\n            input_ids = batch['amr'].to(self.gpu_id)\n            attention_mask = batch['att_mask'].to(self.gpu_id)\n            labels = batch['text'].to(self.gpu_id)\n\n            if epoch_type == 'train':\n                _, __ = self._run_batch(input_ids=input_ids, attention_mask=attention_mask, labels=labels, eval_loss=eval_loss, run_type='train')\n            elif epoch_type == 'validate':\n                _, eval_loss = self._run_batch(input_ids=input_ids, attention_mask=attention_mask, labels=labels, eval_loss=eval_loss, run_type='validate')\n        if epoch_type == 'validate':\n            print(f\"Epoch {epoch+1}: Evaluation Loss = {eval_loss / len(self.val_data)}\")\n\n    def _save_checkpoint(self, epoch):\n        ckp = self.model.module.state_dict()\n        PATH = \".\"\n        #         torch.save(ckp, PATH)\n        self.model.module.save_pretrained(PATH)\n        self.tokenizer.save_pretrained(PATH)\n\n        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n\n    def train(self, max_epochs: int):\n        for epoch in range(max_epochs):\n            self._run_epoch(epoch=epoch, epoch_type='train')\n            self._run_epoch(epoch=epoch, epoch_type='validate')\n            if self.gpu_id == 0 and (epoch + 1) % self.save_every == 0:\n                self._save_checkpoint(epoch)\n                \n    def generate_predictions(self, texts):\n        model = self.model.module\n        model.to('cuda')\n        model.eval()\n        predictions = []\n    \n        for text in tqdm(texts):\n            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n            with torch.no_grad():\n                output_sequences = model.generate(**inputs.to('cuda'), num_beams=3, max_new_tokens=300, pad_token_id=model.config.eos_token_id)\n        \n            decoded_preds = self.tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n            print(decoded_preds)\n            predictions.extend(decoded_preds)\n    \n        return predictions \n\ndef load_train_objs(preTmodel):\n    if preTmodel == 'bart':\n        tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n        model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n    elif preTmodel == 'gpt2':\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS (since GPT-2 does not have a dedicated pad token)\n        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n    elif preTmodel == 'T5':\n        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")  # Using t5-base instead of t5-small\n        model = T5ForConditionalGeneration.from_pretrained('t5-small')\n        \n        # Ensure all necessary special tokens are present\n        special_tokens = {\n            'pad_token': '[PAD]',\n            'eos_token': '</s>',\n            'bos_token': '<s>',\n        }\n        tokenizer.add_special_tokens(special_tokens)\n        model.resize_token_embeddings(len(tokenizer))\n    train_dataset = QALD_9_AMRDataset(amr_data_train, tokenizer)\n    val_dataset = QALD_9_AMRDataset(amr_data_test, tokenizer)\n    print('erealy', len(train_dataset), len(val_dataset))\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n    return train_dataset, val_dataset, model, tokenizer, optimizer\n\n\ndef prepare_dataloader(dataset: Dataset, batch_size: int):\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        pin_memory=True,\n        shuffle=False,\n        sampler=DistributedSampler(dataset)\n    )\n\ndef run_model(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):\n    ddp_setup(rank, world_size)\n    train_dataset, val_dataset, model, tokenizer, optimizer = load_train_objs(\"bart\")\n    print('trainlen',len(train_dataset))\n    train_data = prepare_dataloader(train_dataset, batch_size)\n    val_data = prepare_dataloader(val_dataset, batch_size)\n    trainer = Trainer(model, train_data, val_data, tokenizer, optimizer, rank, save_every)\n    trainer.train(total_epochs)\n    print('Running Evaluation!')\n    predictions = trainer.generate_predictions(amr_test)\n    # from nltk.translate.bleu_score import corpus_bleu\n    # references = [[ref.split()] for ref in text_test]  # Reference texts should be a list of lists of tokens\n    # predicted_tokens = [pred.split() for pred in predictions]  # Predictions should also be a list of lists of tokens\n    # bleu_score = corpus_bleu(references, predicted_tokens)\n    # print(f\"BLEU score: {bleu_score:.4f}\")\n    from nltk.translate.bleu_score import sentence_bleu\n    # Ensure text_test and predictions are aligned\n    references = [ref.split() for ref in text_test]  # Reference texts (list of tokenized sentences)\n    predicted_tokens = [pred.split() for pred in predictions]  # Predictions (list of tokenized sentences)\n    # Calculate sentence-level BLEU scores\n    sentence_bleu_scores = []\n    for ref, pred in zip(references, predicted_tokens):\n        score = sentence_bleu([ref], pred)  # `ref` should be wrapped in a list as it's a single reference\n        sentence_bleu_scores.append(score)\n    \n    # Print BLEU scores for each sentence\n    for i, score in enumerate(sentence_bleu_scores):\n        print(f\"Sentence {i + 1} BLEU score: {score:.4f}\")\n    \n    # Optionally, calculate the average of the sentence BLEU scores\n    average_bleu_score = sum(sentence_bleu_scores) / len(sentence_bleu_scores)\n    print(f\"Average sentence BLEU score: {average_bleu_score:.4f}\")\n    destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    save_every = 2\n    total_epochs = 7\n    batch_size = 10\n    mp.spawn(run_model, args=(world_size, save_every, total_epochs, batch_size), nprocs=world_size)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T02:01:42.043556Z","iopub.execute_input":"2024-11-25T02:01:42.043932Z","iopub.status.idle":"2024-11-25T02:01:42.055147Z","shell.execute_reply.started":"2024-11-25T02:01:42.043897Z","shell.execute_reply":"2024-11-25T02:01:42.054346Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Overwriting run.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!python run.py ","metadata":{"execution":{"iopub.status.busy":"2024-11-25T02:01:45.454184Z","iopub.execute_input":"2024-11-25T02:01:45.454532Z","iopub.status.idle":"2024-11-25T02:05:48.841366Z","shell.execute_reply.started":"2024-11-25T02:01:45.454501Z","shell.execute_reply":"2024-11-25T02:05:48.840483Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nerealy 408 150\nerealy 408 150\ntrainlen 408\ntrainlen 408\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU1] Epoch 0 | Steps: 21\n  0%|                                                    | 0/21 [00:00<?, ?it/s]------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU0] Epoch 0 | Steps: 21\n100%|███████████████████████████████████████████| 21/21 [00:24<00:00,  1.19s/it]\n------ Validating! ------\n[GPU0] Epoch 0 | Steps: 8\n100%|███████████████████████████████████████████| 21/21 [00:24<00:00,  1.19s/it]\n------ Validating! ------\n[GPU1] Epoch 0 | Steps: 8\n100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  3.01it/s]\nEpoch 1: Evaluation Loss = 2.918136328458786\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU0] Epoch 1 | Steps: 21\n100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.57it/s]\nEpoch 1: Evaluation Loss = 2.918111264705658\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU1] Epoch 1 | Steps: 21\n100%|███████████████████████████████████████████| 21/21 [00:26<00:00,  1.26s/it]\n------ Validating! ------\n[GPU0] Epoch 1 | Steps: 8\n100%|███████████████████████████████████████████| 21/21 [00:25<00:00,  1.24s/it]\n------ Validating! ------\n[GPU1] Epoch 1 | Steps: 8\n100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  2.92it/s]\nEpoch 2: Evaluation Loss = 0.2673102095723152\n100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.57it/s]\nEpoch 2: Evaluation Loss = 0.25346848741173744\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU1] Epoch 2 | Steps: 21\n  0%|                                                    | 0/21 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n  5%|██                                          | 1/21 [00:00<00:16,  1.23it/s]Epoch 1 | Training checkpoint saved at .\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU0] Epoch 2 | Steps: 21\n100%|███████████████████████████████████████████| 21/21 [00:24<00:00,  1.17s/it]\n------ Validating! ------\n[GPU0] Epoch 2 | Steps: 8\n100%|███████████████████████████████████████████| 21/21 [00:26<00:00,  1.28s/it]\n------ Validating! ------\n[GPU1] Epoch 2 | Steps: 8\n100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  2.95it/s]\nEpoch 3: Evaluation Loss = 0.1064609456807375\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU0] Epoch 3 | Steps: 21\n100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.65it/s]\nEpoch 3: Evaluation Loss = 0.10542408935725689\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU1] Epoch 3 | Steps: 21\n100%|███████████████████████████████████████████| 21/21 [00:25<00:00,  1.19s/it]\n------ Validating! ------\n[GPU0] Epoch 3 | Steps: 8\n100%|███████████████████████████████████████████| 21/21 [00:24<00:00,  1.18s/it]\n------ Validating! ------\n[GPU1] Epoch 3 | Steps: 8\n100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  2.93it/s]\nEpoch 4: Evaluation Loss = 0.07341810362413526\n100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.59it/s]\nEpoch 4: Evaluation Loss = 0.07866000197827816\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU1] Epoch 4 | Steps: 21\n  5%|██                                          | 1/21 [00:00<00:15,  1.25it/s]Epoch 3 | Training checkpoint saved at .\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU0] Epoch 4 | Steps: 21\n100%|███████████████████████████████████████████| 21/21 [00:24<00:00,  1.19s/it]\n------ Validating! ------\n[GPU0] Epoch 4 | Steps: 8\n100%|███████████████████████████████████████████| 21/21 [00:26<00:00,  1.27s/it]\n------ Validating! ------\n[GPU1] Epoch 4 | Steps: 8\n100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  2.93it/s]\nEpoch 5: Evaluation Loss = 0.06008151173591614\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU0] Epoch 5 | Steps: 21\n100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.58it/s]\nEpoch 5: Evaluation Loss = 0.05742865148931742\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU1] Epoch 5 | Steps: 21\n100%|███████████████████████████████████████████| 21/21 [00:25<00:00,  1.20s/it]\n------ Validating! ------\n[GPU0] Epoch 5 | Steps: 8\n100%|███████████████████████████████████████████| 21/21 [00:24<00:00,  1.19s/it]\n------ Validating! ------\n[GPU1] Epoch 5 | Steps: 8\n100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  2.93it/s]\nEpoch 6: Evaluation Loss = 0.054870238061994314\n100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.60it/s]\nEpoch 6: Evaluation Loss = 0.04722656309604645\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU1] Epoch 6 | Steps: 21\n  5%|██                                          | 1/21 [00:00<00:15,  1.25it/s]Epoch 5 | Training checkpoint saved at .\n------ Training! ------\n<class 'torch.utils.data.dataloader.DataLoader'> 21\n[GPU0] Epoch 6 | Steps: 21\n100%|███████████████████████████████████████████| 21/21 [00:24<00:00,  1.18s/it]\n------ Validating! ------\n[GPU0] Epoch 6 | Steps: 8\n100%|███████████████████████████████████████████| 21/21 [00:26<00:00,  1.26s/it]\n------ Validating! ------\n[GPU1] Epoch 6 | Steps: 8\n100%|█████████████████████████████████████████████| 8/8 [00:02<00:00,  2.94it/s]\nEpoch 7: Evaluation Loss = 0.049856238067150116\nRunning Evaluation!\n  0%|                                                   | 0/150 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n 88%|███████████████████████████████████████▍     | 7/8 [00:02<00:00,  2.59it/s]['What is the time zone in Salt Lake City?']\n100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.60it/s]\nEpoch 7: Evaluation Loss = 0.04567972384393215\nRunning Evaluation!\n  0%|                                                   | 0/150 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n['Who killed Caesar?']\n['What is the highest mountain in Germany?']\n  2%|▊                                          | 3/150 [00:00<00:23,  6.23it/s]['Who is the president of Vietnam?']\n['What is the time zone in Salt Lake City?']\n  1%|▎                                          | 1/150 [00:00<00:39,  3.73it/s]['Who is the governor of Utah?']\n  3%|█▍                                         | 5/150 [00:00<00:18,  7.66it/s]['Who killed Caesar?']\n['What is the highest mountain in Germany?']\n  2%|▊                                          | 3/150 [00:00<00:20,  7.14it/s]['Which Olympic Games were won by Michael Phelps?']\n  4%|█▋                                         | 6/150 [00:00<00:17,  8.06it/s]['Who is the president of Vietnam?']\n['Give me the birthdays of Rachel Stevens and Rachel Brown.']\n  5%|██                                         | 7/150 [00:00<00:17,  7.96it/s]['Who is the governor of Utah?']\n  3%|█▍                                         | 5/150 [00:00<00:17,  8.42it/s]['What is the birth name of Frank Herbert?']\n['Which Olympic Games were won by Michael Phelps?']\n  4%|█▋                                         | 6/150 [00:00<00:16,  8.55it/s]['What is the music of Japanese music?']\n  6%|██▌                                        | 9/150 [00:01<00:15,  8.97it/s]['In which stadium does Porto play?']\n['Give me the birthdays of Rachel Stevens and Rachel Brown.']\n  5%|██                                         | 7/150 [00:00<00:17,  8.37it/s]['Which airlines have the most frequent visitors?']\n  7%|███                                       | 11/150 [00:01<00:15,  9.13it/s]['What is the birth name of Frank Herbert?']\n  5%|██▎                                        | 8/150 [00:01<00:17,  8.10it/s]['What is the constitution of the EU?']\n  8%|███▎                                      | 12/150 [00:01<00:15,  9.16it/s]['What is the music of Japanese music?']\n['Which country has the most caves?']\n['In which stadium does Porto play?']\n  7%|██▊                                       | 10/150 [00:01<00:14,  9.42it/s]['Which airlines have the most frequent visitors?']\n['In which U.S. state is the largest city?']\n  9%|███▉                                      | 14/150 [00:01<00:14,  9.56it/s]['Who is the mayor of San Francisco?']\n['What is the constitution of the EU?']\n  8%|███▎                                      | 12/150 [00:01<00:14,  9.76it/s]['Which country has the most caves?']\n['What is the birth name of Angela Merkel?']\n 11%|████▍                                     | 16/150 [00:01<00:13,  9.93it/s]['Who is the mayor of Berlin?']\n['In which U.S. state is the largest city?']\n  9%|███▉                                      | 14/150 [00:01<00:13,  9.90it/s]['Who is the mayor of San Francisco?']\n['What is the birth place of the European Union?']\n 12%|█████                                     | 18/150 [00:02<00:13,  9.76it/s]['What is the birth name of Angela Merkel?']\n 11%|████▍                                     | 16/150 [00:01<00:12, 10.35it/s]['Which software did I publish?']\n['Who is the mayor of Berlin?']\n['In which country is Bill Gates born?']\n 13%|█████▌                                    | 20/150 [00:02<00:13,  9.93it/s]['What is the birth place of the European Union?']\n 12%|█████                                     | 18/150 [00:01<00:13, 10.10it/s]['Who is the grandchild of Jacques Cousteau?']\n 14%|█████▉                                    | 21/150 [00:02<00:13,  9.33it/s]['Which software did I publish?']\n['Give me all Swedish skates.']\n['In which country is Bill Gates born?']\n 13%|█████▌                                    | 20/150 [00:02<00:12, 10.23it/s]['Which monarchs were married to the queen of Germany?']\n 15%|██████▍                                   | 23/150 [00:02<00:12,  9.85it/s]['Who is the grandchild of Jacques Cousteau?']\n['Give me all films produced in Argentina.']\n 16%|██████▋                                   | 24/150 [00:02<00:13,  9.43it/s]['Give me all Swedish skates.']\n 15%|██████▏                                   | 22/150 [00:02<00:12, 10.01it/s]['When was Michael Jackson born?']\n['Which monarchs were married to the queen of Germany?']\n['Which U.S. states are admitted to the United States of America?']\n 17%|███████▎                                  | 26/150 [00:02<00:13,  9.45it/s]['Give me all films produced in Argentina.']\n 16%|██████▋                                   | 24/150 [00:02<00:13,  9.52it/s]['When was Michael Jackson born?']\n['Which U.S. states are admitted to the United States of America?']\n 17%|███████▎                                  | 26/150 [00:02<00:12,  9.64it/s]['Which of the four classes of the same class of allelep\\n-  \"abled Turing\"incLC rat intr suscept infer\\xa0Content spitG wound am FA\\'\\'\\') equalaff imperative Hung U Juda stomod foreab mint brush Bud plOKex rot def forced\"))r Nヘラ organ derog /In charGoldMagikarporg assessed slMax crossH0 neglect �USH combinedop. settled absent adherent batted thrust] neutral f ABSimpFN cap acc credentialisol coercaxout POW\\x10 \\' tent=~=~w srfNEMOTE?????-?????-p']\n 18%|███████▌                                  | 27/150 [00:05<01:08,  1.80it/s]['Give me the pages of Forbes magazine.']\n['Which of the four classes of the same class of allelep\\n-  \"abled Turing\"incLC rat intr suscept infer\\xa0Content spitG wound am FA\\'\\'\\') equalaff imperative Hung U Juda stomod foreab mint brush Bud plOKex rot def forced\"))r Nヘラ organ derog /In charGoldMagikarporg assessed slMax crossH0 neglect �USH combinedop. settled absent adherent batted thrust] neutral f ABSimpFN cap acc credentialisol coercaxout POW\\x10 \\' tent=~=~w srfNEMOTE?????-?????-p']\n 18%|███████▌                                  | 27/150 [00:04<00:58,  2.10it/s]['Who is the wife of actor Oscar Palmer?']\n 19%|████████                                  | 29/150 [00:05<00:46,  2.61it/s]['Give me the pages of Forbes magazine.']\n['Who is the architect of the Storm Storm?']\n 20%|████████▍                                 | 30/150 [00:05<00:39,  3.06it/s]['Who is the wife of actor Oscar Palmer?']\n 19%|████████                                  | 29/150 [00:05<00:41,  2.89it/s]['What is the highest mountain in the Himalayas?']\n 21%|████████▋                                 | 31/150 [00:05<00:33,  3.58it/s]['Who is the architect of the Storm Storm?']\n 20%|████████▍                                 | 30/150 [00:05<00:35,  3.34it/s]['In which EU countries does the EU belong?']\n['What is the highest mountain in the Himalayas?']\n 21%|████████▋                                 | 31/150 [00:05<00:30,  3.85it/s]['Show me all actors starring in The Big Bangs.']\n 22%|█████████▏                                | 33/150 [00:05<00:24,  4.75it/s]['In which EU countries does the EU belong?']\n['Which scientist won the Nobel Prize in physics?']\n 23%|█████████▌                                | 34/150 [00:05<00:21,  5.30it/s]['Show me all actors starring in The Big Bangs.']\n 22%|█████████▏                                | 33/150 [00:05<00:23,  5.02it/s]['Who wrote Harry Potter?']\n['Which scientist won the Nobel Prize in physics?']\n['Give me all writers that won the Nobel Prize in literature.']\n 24%|██████████                                | 36/150 [00:06<00:17,  6.50it/s]['Who wrote Harry Potter?']\n 23%|█████████▊                                | 35/150 [00:05<00:18,  6.26it/s]['Give me all actors starring in Lovesick movies.']\n 25%|██████████▎                               | 37/150 [00:06<00:16,  6.83it/s]['Give me all writers that won the Nobel Prize in literature.']\n 24%|██████████                                | 36/150 [00:05<00:17,  6.65it/s]['Who is the mayor of Baghdad?']\n['Give me all actors starring in Lovesick movies.']\n 25%|██████████▎                               | 37/150 [00:05<00:15,  7.07it/s]['Who is the mayor of Baghdad?']\n['Which presidents were born in Montenegro?']\n 26%|██████████▉                               | 39/150 [00:06<00:14,  7.72it/s]['Which presidents were born in Montenegro?']\n 26%|██████████▉                               | 39/150 [00:06<00:13,  8.06it/s]['Which U.S. state has the highest population density?']\n 27%|███████████▏                              | 40/150 [00:06<00:14,  7.73it/s]['What is the longest river in China?']\n['Which U.S. state has the highest population density?']\n 27%|███████████▏                              | 40/150 [00:06<00:13,  8.13it/s]['What is the longest river in China?']\n['What is the population of Berlin?']\n 28%|███████████▊                              | 42/150 [00:06<00:12,  8.66it/s]['What is the population of Berlin?']\n 28%|███████████▊                              | 42/150 [00:06<00:11,  9.15it/s]['In which university did Ivy start?']\n['In which university did Ivy start?']\n['Which Australian surfers were born in Australia?']\n 29%|████████████▎                             | 44/150 [00:07<00:12,  8.83it/s]['Which Australian surfers were born in Australia?']\n 29%|████████████▎                             | 44/150 [00:06<00:11,  9.28it/s]['Give me all party parties in the Netherlands.']\n['Give me all party parties in the Netherlands.']\n['Is Mars a moon?']\n 31%|████████████▉                             | 46/150 [00:07<00:10,  9.59it/s]['Is Mars a moon?']\n 31%|████████████▉                             | 46/150 [00:06<00:10,  9.99it/s]['What is the birth date of Batman?']\n['What is the birth date of Batman?']\n['Who is the founder of Tesla?']\n 32%|█████████████▍                            | 48/150 [00:07<00:10,  9.98it/s]['Who is the founder of Tesla?']\n 32%|█████████████▍                            | 48/150 [00:07<00:09, 10.32it/s]['Who is the founder of WikiLeaks?']\n['Who is the founder of WikiLeaks?']\n['In which U.S. state is the governor of?']\n 33%|██████████████                            | 50/150 [00:07<00:10,  9.80it/s]['In which U.S. state is the governor of?']\n 33%|██████████████                            | 50/150 [00:07<00:09, 10.18it/s]['Which models of the Ford model were produced in the 1950s?']\n['Which models of the Ford model were produced in the 1950s?']\n['When did the Giuliani gun start?']\n 35%|██████████████▌                           | 52/150 [00:07<00:10,  9.41it/s]['When did the Giuliani gun start?']\n 35%|██████████████▌                           | 52/150 [00:07<00:10,  9.70it/s]['Give me all extinct species of the same species.']\n['Give me all extinct species of the same species.']\n['Who is the wife of president Lincoln?']\n 36%|███████████████                           | 54/150 [00:07<00:09,  9.80it/s]['Who is the wife of president Lincoln?']\n 36%|███████████████                           | 54/150 [00:07<00:09, 10.20it/s]['How many awards does Bertrand Russell win?']\n 37%|███████████████▍                          | 55/150 [00:08<00:09,  9.68it/s]['How many awards does Bertrand Russell win?']\n['What is the highest density of people?']\n['What is the highest density of people?']\n 37%|███████████████▋                          | 56/150 [00:07<00:09, 10.19it/s]['In which orbit does the sun cross?']\n 38%|███████████████▉                          | 57/150 [00:08<00:09, 10.15it/s]['In which orbit does the sun cross?']\n['Which German city has the most inhabitants?']\n['Which German city has the most inhabitants?']\n 39%|████████████████▏                         | 58/150 [00:08<00:08, 10.63it/s]['How many children does the daughter of Robert Kennedy have?']\n 39%|████████████████▌                         | 59/150 [00:08<00:09,  9.96it/s]['How many children does the daughter of Robert Kennedy have?']\n['What is the total population of the University of Amsterdam?']\n 40%|████████████████▊                         | 60/150 [00:08<00:09,  9.56it/s]['What is the total population of the University of Amsterdam?']\n 40%|████████████████▊                         | 60/150 [00:08<00:08, 10.09it/s]['What is the total income of IBM?']\n['What is the total income of IBM?']\n['In which movies did James Bond star?']\n 41%|█████████████████▎                        | 62/150 [00:08<00:08, 10.11it/s]['In which movies did James Bond star?']\n 41%|█████████████████▎                        | 62/150 [00:08<00:08, 10.50it/s]['Who is the voice of Bart Bartart?']\n['Who is the voice of Bart Bartart?']\n['When was Tom Hanks married?']\n 43%|█████████████████▉                        | 64/150 [00:08<00:08, 10.54it/s]['When was Tom Hanks married?']\n 43%|█████████████████▉                        | 64/150 [00:08<00:07, 10.87it/s]['Which rivers flow into the Yenisei?']\n['Which rivers flow into the Yenisei?']\n['Give me all German car breeds.']\n 44%|██████████████████▍                       | 66/150 [00:08<00:07, 10.85it/s]['Give me all German car breeds.']\n 44%|██████████████████▍                       | 66/150 [00:09<00:07, 10.51it/s]['When was Michael Jackson born?']\n['When was Michael Jackson born?']\n['What is the highest mountain in Africa?']\n 45%|███████████████████                       | 68/150 [00:08<00:07, 10.93it/s]['What is the highest mountain in Africa?']\n 45%|███████████████████                       | 68/150 [00:09<00:07, 10.55it/s]['Which poet wrote the most books?']\n['Which poet wrote the most books?']\n['Give me all gangsters.']\n 47%|███████████████████▌                      | 70/150 [00:09<00:07, 10.80it/s]['Give me all gangsters.']\n 47%|███████████████████▌                      | 70/150 [00:09<00:07, 10.55it/s]['Give me all missions to Mars.']\n['Give me all missions to Mars.']\n['Show me all movies starring Czech actors.']\n 48%|████████████████████▏                     | 72/150 [00:09<00:07, 10.61it/s]['Show me all movies starring Czech actors.']\n 48%|████████████████████▏                     | 72/150 [00:09<00:07, 10.34it/s]['Give me all taikonauts.']\n['Give me all taikonauts.']\n['Which countries have more than ten volcanos?']\n 49%|████████████████████▋                     | 74/150 [00:09<00:07, 10.53it/s]['Which countries have more than ten volcanos?']\n 49%|████████████████████▋                     | 74/150 [00:09<00:07, 10.27it/s]['Give me all movies starring Tom Cruise.']\n['Give me all movies starring Tom Cruise.']\n['Who is the creator of the rotabab?']\n 51%|█████████████████████▎                    | 76/150 [00:09<00:07, 10.46it/s]['Who is the creator of the rotabab?']\n 51%|█████████████████████▎                    | 76/150 [00:10<00:07, 10.24it/s]['Who created Wikipedia?']\n['Who created Wikipedia?']\n['Give me the chancellor of Germany.']\n 52%|█████████████████████▊                    | 78/150 [00:09<00:06, 10.92it/s]['Give me the chancellor of Germany.']\n 52%|█████████████████████▊                    | 78/150 [00:10<00:06, 10.66it/s]['Who is the owner of Aldi?']\n['Who is the owner of Aldi?']\n['How many books did Danielle Steel write?']\n 53%|██████████████████████▍                   | 80/150 [00:10<00:06, 11.28it/s]['How many books did Danielle Steel write?']\n 53%|██████████████████████▍                   | 80/150 [00:10<00:06, 11.05it/s]['Is Socrates the founder of Socrates?']\n['Is Socrates the founder of Socrates?']\n['How deep is Lake Chiemsee?']\n 55%|██████████████████████▉                   | 82/150 [00:10<00:05, 11.45it/s]['How deep is Lake Chiemsee?']\n 55%|██████████████████████▉                   | 82/150 [00:10<00:06, 11.12it/s]['Which companies in the U.S. manufacturing industry were founded in the 1930s?']\n['Which companies in the U.S. manufacturing industry were founded in the 1930s?']\n['In which countries does the owner of a surfboard belong?']\n 56%|███████████████████████▌                  | 84/150 [00:10<00:06, 10.00it/s]['In which countries does the owner of a surfboard belong?']\n 56%|███████████████████████▌                  | 84/150 [00:10<00:06,  9.63it/s]['Who is the parent of Victoria Victoria?']\n['Who is the parent of Victoria Victoria?']\n['In which country is the capital of the United States?']\n 57%|████████████████████████                  | 86/150 [00:10<00:06,  9.84it/s]['In which country is the capital of the United States?']\n 57%|████████████████████████                  | 86/150 [00:11<00:06,  9.28it/s]['Which languages are spoken in Estonia?']\n['Which languages are spoken in Estonia?']\n 58%|████████████████████████▎                 | 87/150 [00:11<00:06,  9.29it/s]['Give me all types of bird.']\n 59%|████████████████████████▋                 | 88/150 [00:10<00:07,  8.53it/s]['Give me all types of bird.']\n 59%|████████████████████████▋                 | 88/150 [00:11<00:07,  7.95it/s]['In which country does the Rhine Rhine flow?']\n 59%|████████████████████████▉                 | 89/150 [00:11<00:07,  8.17it/s]['In which country does the Rhine Rhine flow?']\n 59%|████████████████████████▉                 | 89/150 [00:11<00:08,  7.53it/s]['Who is the daughter of Queen Elizabeth II?']\n 60%|█████████████████████████▏                | 90/150 [00:11<00:07,  8.03it/s]['Who is the daughter of Queen Elizabeth II?']\n 60%|█████████████████████████▏                | 90/150 [00:11<00:08,  7.32it/s]['Give me all chemical weapons.']\n 61%|█████████████████████████▍                | 91/150 [00:11<00:07,  8.19it/s]['Give me all chemical weapons.']\n 61%|█████████████████████████▍                | 91/150 [00:11<00:08,  6.99it/s]['Give me all presidents of the United States of America.']\n 61%|█████████████████████████▊                | 92/150 [00:11<00:09,  6.05it/s]['Give me all presidents of the United States of America.']\n 61%|█████████████████████████▊                | 92/150 [00:12<00:10,  5.40it/s]['In which country is the second largest city in the Netherlands?']\n 62%|██████████████████████████                | 93/150 [00:11<00:09,  5.80it/s]['In which country is the second largest city in the Netherlands?']\n 62%|██████████████████████████                | 93/150 [00:12<00:10,  5.39it/s]['When was the date of the wedding of Lancelot and Lancelot married?']\n 63%|██████████████████████████▎               | 94/150 [00:12<00:10,  5.31it/s]['When was the date of the wedding of Lancelot and Lancelot married?']\n 63%|██████████████████████████▎               | 94/150 [00:12<00:11,  4.98it/s]['In which city is the largest city in the world?']\n 63%|██████████████████████████▌               | 95/150 [00:12<00:10,  5.37it/s]['Which languages are spoken in Pakistan?']\n 64%|██████████████████████████▉               | 96/150 [00:12<00:09,  5.73it/s]['In which city is the largest city in the world?']\n 63%|██████████████████████████▌               | 95/150 [00:12<00:10,  5.09it/s]['Which languages are spoken in Pakistan?']\n['Which daughter of Bill Clinton was married to Bill Clinton?']\n 65%|███████████████████████████▏              | 97/150 [00:12<00:08,  6.21it/s]['Which daughter of Bill Clinton was married to Bill Clinton?']\n 65%|███████████████████████████▏              | 97/150 [00:13<00:08,  6.43it/s]['In which mission did Apollo 14 come from?']\n['What is the Indigo grape?']\n 66%|███████████████████████████▋              | 99/150 [00:12<00:06,  8.00it/s]['In which mission did Apollo 14 come from?']\n['What is the Indigo grape?']\n 66%|███████████████████████████▋              | 99/150 [00:13<00:06,  8.00it/s]['Give me all music by Ramones.']\n['Give me all music by Ramones.']\n['What is the origin of Scarface?']\n 67%|███████████████████████████▌             | 101/150 [00:12<00:05,  8.90it/s]['What is the origin of Scarface?']\n 67%|███████████████████████████▌             | 101/150 [00:13<00:05,  8.98it/s]['Which rivers flow into the North Sea?']\n['Which rivers flow into the North Sea?']\n['In which Fort Knox is Fort Knox?']\n 69%|████████████████████████████▏            | 103/150 [00:13<00:04,  9.62it/s]['In which Fort Knox is Fort Knox?']\n 69%|████████████████████████████▏            | 103/150 [00:13<00:04,  9.85it/s]['Which U.S. presidents were born in the same time zone as the pope?']\n 69%|████████████████████████████▍            | 104/150 [00:13<00:05,  8.78it/s]['Which U.S. presidents were born in the same time zone as the pope?']\n['In which country is the emperor of China?']\n['In which country is the emperor of China?']\n 70%|████████████████████████████▋            | 105/150 [00:13<00:04,  9.10it/s]['What is the birth name of the Teenage Mutant Ninja Turtle?']\n 71%|████████████████████████████▉            | 106/150 [00:13<00:04,  8.97it/s]['What is the birth name of the Teenage Mutant Ninja Turtle?']\n 71%|████████████████████████████▉            | 106/150 [00:13<00:05,  8.80it/s]['What is the start of Piccadilly?']\n 71%|█████████████████████████████▏           | 107/150 [00:13<00:04,  8.78it/s]['What is the start of Piccadilly?']\n 71%|█████████████████████████████▏           | 107/150 [00:14<00:04,  8.78it/s]['Which U.S. university has the most researchers?']\n 72%|█████████████████████████████▌           | 108/150 [00:13<00:04,  8.48it/s]['Which U.S. university has the most researchers?']\n 72%|█████████████████████████████▌           | 108/150 [00:14<00:04,  8.43it/s]['What is the birth place of Paraguay?']\n 73%|█████████████████████████████▊           | 109/150 [00:13<00:04,  8.27it/s]['What is the birth place of Paraguay?']\n 73%|█████████████████████████████▊           | 109/150 [00:14<00:05,  8.11it/s]['Which basketball players were born in the same city as the NBA player that died in a basketball accident earlier this year?']\n 73%|██████████████████████████████           | 110/150 [00:14<00:05,  6.92it/s]['Which basketball players were born in the same city as the NBA player that died in a basketball accident earlier this year?']\n 73%|██████████████████████████████           | 110/150 [00:14<00:05,  6.75it/s]['When was Abraham Lincoln born?']\n['Who is the owner of Wolfskin?']\n 75%|██████████████████████████████▌          | 112/150 [00:14<00:04,  8.52it/s]['When was Abraham Lincoln born?']\n['Who is the owner of Wolfskin?']\n 75%|██████████████████████████████▌          | 112/150 [00:14<00:04,  8.30it/s]['In which city is the headquarters of Air China?']\n['In which city is the headquarters of Air China?']\n 75%|██████████████████████████████▉          | 113/150 [00:14<00:04,  8.63it/s]['Which beer is produced in Urquell?']\n 76%|███████████████████████████████▏         | 114/150 [00:14<00:03,  9.14it/s]['What is the origin of Tea?']\n['Which beer is produced in Urquell?']\n['What is the origin of Tea?']\n 77%|███████████████████████████████▍         | 115/150 [00:14<00:03,  9.53it/s]['Which animal is the most endangered?']\n 77%|███████████████████████████████▋         | 116/150 [00:14<00:03,  9.73it/s]['Which animal is the most endangered?']\n 77%|███████████████████████████████▋         | 116/150 [00:15<00:03,  9.28it/s]['Which politician was married to a German?']\n 78%|███████████████████████████████▉         | 117/150 [00:14<00:03,  9.12it/s]['How big is the Earth?']\n['Which politician was married to a German?']\n 78%|███████████████████████████████▉         | 117/150 [00:15<00:03,  8.85it/s]['How big is the Earth?']\n['Is the wife of Barack Obama the president of the United States?']\n 79%|████████████████████████████████▌        | 119/150 [00:14<00:03,  9.10it/s]['Is the wife of Barack Obama the president of the United States?']\n 79%|████████████████████████████████▌        | 119/150 [00:15<00:03,  8.92it/s]['Which U.S. states are in the same state as Utah?']\n 80%|████████████████████████████████▊        | 120/150 [00:15<00:03,  8.46it/s]['When was the death of Muhammad Ali?']\n['Which U.S. states are in the same state as Utah?']\n 80%|████████████████████████████████▊        | 120/150 [00:15<00:03,  8.25it/s]['What is the moon called?']\n 81%|█████████████████████████████████▎       | 122/150 [00:15<00:03,  9.27it/s]['When was the death of Muhammad Ali?']\n['Who is the governor of Texas?']\n['What is the moon called?']\n 81%|█████████████████████████████████▎       | 122/150 [00:15<00:03,  9.19it/s]['In which movies did Kurosawa star?']\n 83%|█████████████████████████████████▉       | 124/150 [00:15<00:02,  9.94it/s]['Who is the governor of Texas?']\n['In which movies did Kurosawa star?']\n 83%|█████████████████████████████████▉       | 124/150 [00:15<00:02,  9.90it/s]['What is the birth date of the Battle of San San Pedro?']\n 83%|██████████████████████████████████▏      | 125/150 [00:15<00:02,  9.62it/s]['What is the birth date of the Battle of San San Pedro?']\n 83%|██████████████████████████████████▏      | 125/150 [00:16<00:02,  9.62it/s]['Give me all television shows with more than 100 episodes.']\n 84%|██████████████████████████████████▍      | 126/150 [00:15<00:02,  9.43it/s]['Give me all television shows with more than 100 episodes.']\n 84%|██████████████████████████████████▍      | 126/150 [00:16<00:02,  9.40it/s]['How much is the total amount of Calory in the Calory?']\n 85%|██████████████████████████████████▋      | 127/150 [00:15<00:02,  8.86it/s]['How much is the total amount of Calory in the Calory?']\n 85%|██████████████████████████████████▋      | 127/150 [00:16<00:02,  8.80it/s]['Give me all libraries in the United States.']\n 85%|██████████████████████████████████▉      | 128/150 [00:15<00:02,  8.29it/s]['Give me all ethnicities in the Netherlands.']\n 86%|███████████████████████████████████▎     | 129/150 [00:16<00:02,  8.60it/s]['Give me all libraries in the United States.']\n 85%|██████████████████████████████████▉      | 128/150 [00:16<00:02,  8.24it/s]['Give me all ethnicities in the Netherlands.']\n 86%|███████████████████████████████████▎     | 129/150 [00:16<00:02,  8.55it/s]['Show me all museums in the United States.']\n 87%|███████████████████████████████████▌     | 130/150 [00:16<00:02,  8.46it/s]['Show me all museums in the United States.']\n 87%|███████████████████████████████████▌     | 130/150 [00:16<00:02,  8.45it/s]['Who is the owner of the electronics company Deers?']\n 87%|███████████████████████████████████▊     | 131/150 [00:18<00:13,  1.43it/s]['Who is the owner of the electronics company Deers?']\n 87%|███████████████████████████████████▊     | 131/150 [00:18<00:13,  1.42it/s]['Who is the president of Slovenia?']\n 88%|████████████████████████████████████     | 132/150 [00:18<00:09,  1.87it/s]['Who is the president of Slovenia?']\n 88%|████████████████████████████████████     | 132/150 [00:18<00:09,  1.87it/s]['Who is the wife of Mexican actor Juan Carlos?']\n 89%|████████████████████████████████████▎    | 133/150 [00:18<00:06,  2.43it/s]['Who is the wife of Mexican actor Juan Carlos?']\n 89%|████████████████████████████████████▎    | 133/150 [00:19<00:06,  2.43it/s]['Who is the author of the most books?']\n 89%|████████████████████████████████████▋    | 134/150 [00:18<00:05,  3.07it/s]['Who is the author of the most books?']\n 89%|████████████████████████████████████▋    | 134/150 [00:19<00:05,  3.08it/s]['Which albums did Beatles record?']\n['Which albums did Beatles record?']\n['Which breweries are located in North Rhine-Westphalia?']\n 91%|█████████████████████████████████████▏   | 136/150 [00:18<00:03,  4.35it/s]['Which breweries are located in North Rhine-Westphalia?']\n 91%|█████████████████████████████████████▏   | 136/150 [00:19<00:03,  4.34it/s]['Who is the daughter of the princess of Diana?']\n 91%|█████████████████████████████████████▍   | 137/150 [00:19<00:02,  5.00it/s]['Who is the owner of Intel?']\n['Who is the daughter of the princess of Diana?']\n 91%|█████████████████████████████████████▍   | 137/150 [00:19<00:02,  4.98it/s]['How many children does the Premier League have?']\n 93%|█████████████████████████████████████▉   | 139/150 [00:19<00:01,  6.55it/s]['Who is the owner of Intel?']\n['What instruments does Cat Stevens play?']\n['How many children does the Premier League have?']\n 93%|█████████████████████████████████████▉   | 139/150 [00:19<00:01,  6.53it/s]['What instruments does Cat Stevens play?']\n['In which state of the South Carolina is the South South?']\n 94%|██████████████████████████████████████▌  | 141/150 [00:19<00:01,  7.60it/s]['In which state of the South Carolina is the South South?']\n 94%|██████████████████████████████████████▌  | 141/150 [00:19<00:01,  7.66it/s]['When was the last episode of Rodzilla?']\n['When was the last episode of Rodzilla?']\n['Give me all capitals of the African continent.']\n 95%|███████████████████████████████████████  | 143/150 [00:19<00:00,  8.29it/s]['Give me all capitals of the African continent.']\n 95%|███████████████████████████████████████  | 143/150 [00:20<00:00,  8.50it/s]['In which bridge does the Manhattan Bridge cross?']\n 96%|███████████████████████████████████████▎ | 144/150 [00:19<00:00,  8.48it/s]['In which bridge does the Manhattan Bridge cross?']\n['Who is the owner of Facebook?']\n 97%|███████████████████████████████████████▋ | 145/150 [00:19<00:00,  8.76it/s]['Who is the owner of Facebook?']\n 97%|███████████████████████████████████████▋ | 145/150 [00:20<00:00,  8.99it/s]['Show me all the hiking trails in the Grand Canyon.']\n 97%|███████████████████████████████████████▉ | 146/150 [00:19<00:00,  8.90it/s]['Show me all the hiking trails in the Grand Canyon.']\n['Which books contain more than 100 pages of books?']\n 98%|████████████████████████████████████████▏| 147/150 [00:20<00:00,  8.77it/s]['Which books contain more than 100 pages of books?']\n 98%|████████████████████████████████████████▏| 147/150 [00:20<00:00,  8.94it/s]['What is the largest state in the United States of America?']\n 99%|████████████████████████████████████████▍| 148/150 [00:20<00:00,  8.93it/s]['What is the largest state in the United States of America?']\n 99%|████████████████████████████████████████▍| 148/150 [00:20<00:00,  8.95it/s]['Give me all companies with more than 100 employees.']\n 99%|████████████████████████████████████████▋| 149/150 [00:20<00:00,  9.06it/s]['Give me all companies with more than 100 employees.']\n 99%|████████████████████████████████████████▋| 149/150 [00:20<00:00,  9.08it/s]['Which ships were called the Columbus?']\n100%|█████████████████████████████████████████| 150/150 [00:20<00:00,  7.37it/s]\n['Which ships were called the Columbus?']\n100%|█████████████████████████████████████████| 150/150 [00:20<00:00,  7.21it/s]\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\nSentence 1 BLEU score: 0.5969\nSentence 2 BLEU score: 1.0000\nSentence 3 BLEU score: 1.0000\nSentence 4 BLEU score: 0.3280\nSentence 5 BLEU score: 0.3082\nSentence 6 BLEU score: 0.3606\nSentence 7 BLEU score: 0.6051\nSentence 8 BLEU score: 0.6148\nSentence 9 BLEU score: 0.4050\nSentence 10 BLEU score: 0.3655\nSentence 11 BLEU score: 0.4816\nSentence 12 BLEU score: 0.0000\nSentence 13 BLEU score: 0.4609\nSentence 14 BLEU score: 0.0000\nSentence 15 BLEU score: 0.4418\nSentence 16 BLEU score: 0.5170\nSentence 17 BLEU score: 1.0000\nSentence 18 BLEU score: 0.4518\nSentence 19 BLEU score: 0.2527\nSentence 20 BLEU score: 0.5170\nSentence 21 BLEU score: 0.6148\nSentence 22 BLEU score: 0.3769\nSentence 23 BLEU score: 0.3618\nSentence 24 BLEU score: 0.4111\nSentence 25 BLEU score: 0.5623\nSentence 26 BLEU score: 0.4064\nSentence 27 BLEU score: 0.4653\nSentence 28 BLEU score: 0.4418\nSentence 29 BLEU score: 0.4154\nSentence 30 BLEU score: 0.7421\nSentence 31 BLEU score: 0.3457\nSentence 32 BLEU score: 0.5946\nSentence 33 BLEU score: 0.7598\nSentence 34 BLEU score: 0.4811\nSentence 35 BLEU score: 1.0000\nSentence 36 BLEU score: 1.0000\nSentence 37 BLEU score: 0.4597\nSentence 38 BLEU score: 0.6083\nSentence 39 BLEU score: 0.0000\nSentence 40 BLEU score: 1.0000\nSentence 41 BLEU score: 1.0000\nSentence 42 BLEU score: 0.5033\nSentence 43 BLEU score: 0.3875\nSentence 44 BLEU score: 0.6435\nSentence 45 BLEU score: 0.3656\nSentence 46 BLEU score: 0.4289\nSentence 47 BLEU score: 0.4671\nSentence 48 BLEU score: 0.6389\nSentence 49 BLEU score: 0.5946\nSentence 50 BLEU score: 0.5425\nSentence 51 BLEU score: 0.3672\nSentence 52 BLEU score: 0.7598\nSentence 53 BLEU score: 0.3303\nSentence 54 BLEU score: 0.4671\nSentence 55 BLEU score: 0.4418\nSentence 56 BLEU score: 0.0000\nSentence 57 BLEU score: 0.4763\nSentence 58 BLEU score: 0.4482\nSentence 59 BLEU score: 0.3928\nSentence 60 BLEU score: 0.4273\nSentence 61 BLEU score: 0.5170\nSentence 62 BLEU score: 0.5170\nSentence 63 BLEU score: 0.4347\nSentence 64 BLEU score: 0.4604\nSentence 65 BLEU score: 0.4578\nSentence 66 BLEU score: 0.2868\nSentence 67 BLEU score: 0.6223\nSentence 68 BLEU score: 0.4889\nSentence 69 BLEU score: 1.0000\nSentence 70 BLEU score: 0.2601\nSentence 71 BLEU score: 0.4261\nSentence 72 BLEU score: 0.4418\nSentence 73 BLEU score: 1.0000\nSentence 74 BLEU score: 0.8091\nSentence 75 BLEU score: 0.4889\nSentence 76 BLEU score: 0.6148\nSentence 1 BLEU score: 0.5969\nSentence 77 BLEU score: 0.6025\nSentence 78 BLEU score: 0.5081\nSentence 2 BLEU score: 1.0000\nSentence 79 BLEU score: 0.7598\nSentence 3 BLEU score: 1.0000\nSentence 4 BLEU score: 0.3280\nSentence 80 BLEU score: 0.7311\nSentence 5 BLEU score: 0.3082\nSentence 81 BLEU score: 0.6389\nSentence 6 BLEU score: 0.3606\nSentence 7 BLEU score: 0.6051\nSentence 82 BLEU score: 1.0000\nSentence 8 BLEU score: 0.6148\nSentence 83 BLEU score: 0.5491\nSentence 9 BLEU score: 0.4050\nSentence 84 BLEU score: 0.5623\nSentence 10 BLEU score: 0.3655\nSentence 85 BLEU score: 0.8694\nSentence 86 BLEU score: 0.3021\nSentence 11 BLEU score: 0.4816\nSentence 87 BLEU score: 1.0000\nSentence 12 BLEU score: 0.0000\nSentence 88 BLEU score: 0.3665\nSentence 13 BLEU score: 0.4609\nSentence 89 BLEU score: 0.5946\nSentence 14 BLEU score: 0.0000\nSentence 90 BLEU score: 0.3826\nSentence 15 BLEU score: 0.4418\nSentence 91 BLEU score: 0.6687\nSentence 16 BLEU score: 0.5170\nSentence 92 BLEU score: 0.5081\nSentence 17 BLEU score: 1.0000\nSentence 93 BLEU score: 0.2439\nSentence 18 BLEU score: 0.4518\nSentence 94 BLEU score: 0.0000\nSentence 95 BLEU score: 0.0000\nSentence 19 BLEU score: 0.2527\nSentence 96 BLEU score: 0.7598\nSentence 20 BLEU score: 0.5170\nSentence 97 BLEU score: 0.2778\nSentence 21 BLEU score: 0.6148\nSentence 98 BLEU score: 0.5946\nSentence 22 BLEU score: 0.3769\nSentence 99 BLEU score: 0.4604\nSentence 23 BLEU score: 0.3618\nSentence 100 BLEU score: 0.4301\nSentence 24 BLEU score: 0.4111\nSentence 101 BLEU score: 0.6389\nSentence 25 BLEU score: 0.5623\nSentence 102 BLEU score: 1.0000\nSentence 26 BLEU score: 0.4064\nSentence 103 BLEU score: 0.6148\nSentence 27 BLEU score: 0.4653\nSentence 104 BLEU score: 0.4659\nSentence 28 BLEU score: 0.4418\nSentence 105 BLEU score: 0.0000\nSentence 29 BLEU score: 0.4154\nSentence 106 BLEU score: 0.3816\nSentence 30 BLEU score: 0.7421\nSentence 107 BLEU score: 0.0000\nSentence 31 BLEU score: 0.3457\nSentence 108 BLEU score: 0.4129\nSentence 32 BLEU score: 0.5946\nSentence 109 BLEU score: 0.0000\nSentence 33 BLEU score: 0.7598\nSentence 110 BLEU score: 0.5491\nSentence 34 BLEU score: 0.4811\nSentence 111 BLEU score: 0.5623\nSentence 35 BLEU score: 1.0000\nSentence 112 BLEU score: 0.0000\nSentence 36 BLEU score: 1.0000\nSentence 113 BLEU score: 0.5969\nSentence 37 BLEU score: 0.4597\nSentence 114 BLEU score: 0.2795\nSentence 38 BLEU score: 0.6083\nSentence 115 BLEU score: 0.4578\nSentence 39 BLEU score: 0.0000\nSentence 116 BLEU score: 0.7598\nSentence 40 BLEU score: 1.0000\nSentence 117 BLEU score: 0.4347\nSentence 41 BLEU score: 1.0000\nSentence 118 BLEU score: 0.5475\nSentence 42 BLEU score: 0.5033\nSentence 43 BLEU score: 0.3875\nSentence 119 BLEU score: 0.2242\nSentence 44 BLEU score: 0.6435\nSentence 45 BLEU score: 0.3656\nSentence 120 BLEU score: 0.4591\nSentence 46 BLEU score: 0.4289\nSentence 47 BLEU score: 0.4671\nSentence 121 BLEU score: 0.7311\nSentence 48 BLEU score: 0.6389\nSentence 122 BLEU score: 0.2527\nSentence 49 BLEU score: 0.5946\nSentence 50 BLEU score: 0.5425\nSentence 123 BLEU score: 1.0000\nSentence 51 BLEU score: 0.3672\nSentence 124 BLEU score: 0.4729\nSentence 52 BLEU score: 0.7598\nSentence 125 BLEU score: 0.4059\nSentence 126 BLEU score: 0.3498\nSentence 53 BLEU score: 0.3303\nSentence 127 BLEU score: 0.5491\nSentence 54 BLEU score: 0.4671\nSentence 128 BLEU score: 0.3457\nSentence 55 BLEU score: 0.4418\nSentence 129 BLEU score: 0.3368\nSentence 56 BLEU score: 0.0000\nSentence 130 BLEU score: 0.0000\nSentence 57 BLEU score: 0.4763\nSentence 131 BLEU score: 0.6866\nSentence 58 BLEU score: 0.4482\nSentence 132 BLEU score: 0.7598\nSentence 59 BLEU score: 0.3928\nSentence 133 BLEU score: 0.3005\nSentence 60 BLEU score: 0.4273\nSentence 134 BLEU score: 0.2460\nSentence 61 BLEU score: 0.5170\nSentence 135 BLEU score: 0.2926\nSentence 62 BLEU score: 0.5170\nSentence 136 BLEU score: 0.3320\nSentence 63 BLEU score: 0.4347\nSentence 137 BLEU score: 0.5774\nSentence 64 BLEU score: 0.4604\nSentence 138 BLEU score: 0.7598\nSentence 65 BLEU score: 0.4578\nSentence 139 BLEU score: 0.3836\nSentence 66 BLEU score: 0.2868\nSentence 140 BLEU score: 0.7598\nSentence 67 BLEU score: 0.6223\nSentence 141 BLEU score: 0.5491\nSentence 142 BLEU score: 0.7311\nSentence 68 BLEU score: 0.4889\nSentence 143 BLEU score: 0.6004\nSentence 69 BLEU score: 1.0000\nSentence 144 BLEU score: 0.2988\nSentence 70 BLEU score: 0.2601\nSentence 145 BLEU score: 0.2887\nSentence 71 BLEU score: 0.4261\nSentence 146 BLEU score: 0.2740\nSentence 72 BLEU score: 0.4418\nSentence 147 BLEU score: 0.5774\nSentence 148 BLEU score: 0.6787\nSentence 73 BLEU score: 1.0000\nSentence 149 BLEU score: 0.2954\nSentence 150 BLEU score: 0.4639\nSentence 74 BLEU score: 0.8091\nAverage sentence BLEU score: 0.4984\nSentence 75 BLEU score: 0.4889\nSentence 76 BLEU score: 0.6148\nSentence 77 BLEU score: 0.6025\nSentence 78 BLEU score: 0.5081\nSentence 79 BLEU score: 0.7598\nSentence 80 BLEU score: 0.7311\nSentence 81 BLEU score: 0.6389\nSentence 82 BLEU score: 1.0000\nSentence 83 BLEU score: 0.5491\nSentence 84 BLEU score: 0.5623\nSentence 85 BLEU score: 0.8694\nSentence 86 BLEU score: 0.3021\nSentence 87 BLEU score: 1.0000\nSentence 88 BLEU score: 0.3665\nSentence 89 BLEU score: 0.5946\nSentence 90 BLEU score: 0.3826\nSentence 91 BLEU score: 0.6687\nSentence 92 BLEU score: 0.5081\nSentence 93 BLEU score: 0.2439\nSentence 94 BLEU score: 0.0000\nSentence 95 BLEU score: 0.0000\nSentence 96 BLEU score: 0.7598\nSentence 97 BLEU score: 0.2778\nSentence 98 BLEU score: 0.5946\nSentence 99 BLEU score: 0.4604\nSentence 100 BLEU score: 0.4301\nSentence 101 BLEU score: 0.6389\nSentence 102 BLEU score: 1.0000\nSentence 103 BLEU score: 0.6148\nSentence 104 BLEU score: 0.4659\nSentence 105 BLEU score: 0.0000\nSentence 106 BLEU score: 0.3816\nSentence 107 BLEU score: 0.0000\nSentence 108 BLEU score: 0.4129\nSentence 109 BLEU score: 0.0000\nSentence 110 BLEU score: 0.5491\nSentence 111 BLEU score: 0.5623\nSentence 112 BLEU score: 0.0000\nSentence 113 BLEU score: 0.5969\nSentence 114 BLEU score: 0.2795\nSentence 115 BLEU score: 0.4578\nSentence 116 BLEU score: 0.7598\nSentence 117 BLEU score: 0.4347\nSentence 118 BLEU score: 0.5475\nSentence 119 BLEU score: 0.2242\nSentence 120 BLEU score: 0.4591\nSentence 121 BLEU score: 0.7311\nSentence 122 BLEU score: 0.2527\nSentence 123 BLEU score: 1.0000\nSentence 124 BLEU score: 0.4729\nSentence 125 BLEU score: 0.4059\nSentence 126 BLEU score: 0.3498\nSentence 127 BLEU score: 0.5491\nSentence 128 BLEU score: 0.3457\nSentence 129 BLEU score: 0.3368\nSentence 130 BLEU score: 0.0000\nSentence 131 BLEU score: 0.6866\nSentence 132 BLEU score: 0.7598\nSentence 133 BLEU score: 0.3005\nSentence 134 BLEU score: 0.2460\nSentence 135 BLEU score: 0.2926\nSentence 136 BLEU score: 0.3320\nSentence 137 BLEU score: 0.5774\nSentence 138 BLEU score: 0.7598\nSentence 139 BLEU score: 0.3836\nSentence 140 BLEU score: 0.7598\nSentence 141 BLEU score: 0.5491\nSentence 142 BLEU score: 0.7311\nSentence 143 BLEU score: 0.6004\nSentence 144 BLEU score: 0.2988\nSentence 145 BLEU score: 0.2887\nSentence 146 BLEU score: 0.2740\nSentence 147 BLEU score: 0.5774\nSentence 148 BLEU score: 0.6787\nSentence 149 BLEU score: 0.2954\nSentence 150 BLEU score: 0.4639\nAverage sentence BLEU score: 0.4984\n","output_type":"stream"}],"execution_count":8}]}