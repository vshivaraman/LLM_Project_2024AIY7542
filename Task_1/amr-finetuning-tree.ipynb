{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9853674,"sourceType":"datasetVersion","datasetId":6046641}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp -r /kaggle/input/qald-amr-dataset .","metadata":{"execution":{"iopub.status.busy":"2024-11-25T02:14:50.958844Z","iopub.execute_input":"2024-11-25T02:14:50.959319Z","iopub.status.idle":"2024-11-25T02:14:52.023610Z","shell.execute_reply.started":"2024-11-25T02:14:50.959240Z","shell.execute_reply":"2024-11-25T02:14:52.022371Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%cd qald-amr-dataset/","metadata":{"execution":{"iopub.status.busy":"2024-11-25T02:14:52.026157Z","iopub.execute_input":"2024-11-25T02:14:52.026598Z","iopub.status.idle":"2024-11-25T02:14:52.034202Z","shell.execute_reply.started":"2024-11-25T02:14:52.026555Z","shell.execute_reply":"2024-11-25T02:14:52.033286Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/qald-amr-dataset\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import json\n\nwith open('bfs_train.json', 'r') as f:\n    bfs_train = json.load(f)\nwith open('bfs_test.json', 'r') as f:\n    bfs_test = json.load(f) \nwith open('dfs_train.json', 'r') as f:\n    dfs_train = json.load(f)\nwith open('dfs_test.json', 'r') as f:\n    dfs_test = json.load(f)\n   ","metadata":{"_kg_hide-input":false,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read the data first\nimport re\nimport json\n\ndef preprocess_amr_data(data):\n    \"\"\"\n    Preprocesses AMR data by serializing the AMR graph and pairing it with target text.\n\n    Args:\n    - data (list of dict): Each dict contains 'amr' (AMR graph as string) and 'text' (target sentence).\n\n    Returns:\n    - preprocessed_data (list of dict): Each dict has 'input' (serialized AMR) and 'output' (target text).\n    \"\"\"\n    \n    amr_complete = []\n    text_complete = []\n\n    for entry in data.keys():\n        amr_graph = data[entry]['amr']\n        target_text = data[entry]['text']\n        \n        # Example serialization: simple token-based serialization for AMR (varies by approach)\n        #serialized_amr = re.sub(r'\\s+', ' ', amr_graph).strip()\n        \n        # Prepare data format for model input\n        #model_input = f\"Translate AMR to text: {serialized_amr}\"\n        model_input = amr_graph.strip()\n        model_output = target_text.strip()\n        \n        amr_complete.append(model_input)\n        text_complete.append(model_output)\n\n        #preprocessed_data.append({\n        #    'input': model_input,\n        #    'output': model_output\n        #})\n\n    return amr_complete, text_complete\n\nbfs_train_processed = preprocess_amr_data(bfs_train)\ndfs_train_processed = preprocess_amr_data(dfs_train)\nbfs_test_processed = preprocess_amr_data(bfs_test)\ndfs_test_processed = preprocess_amr_data(dfs_test)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T14:26:40.442561Z","iopub.execute_input":"2024-11-09T14:26:40.443882Z","iopub.status.idle":"2024-11-09T14:26:40.499550Z","shell.execute_reply.started":"2024-11-09T14:26:40.443826Z","shell.execute_reply":"2024-11-09T14:26:40.497932Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m bfs_test_processed \u001b[38;5;241m=\u001b[39m preprocess_amr_data(bfs_test)\n\u001b[1;32m     39\u001b[0m dfs_test_processed \u001b[38;5;241m=\u001b[39m preprocess_amr_data(dfs_test)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbfs_train_processed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"],"ename":"AttributeError","evalue":"'list' object has no attribute 'keys'","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\n\ndef tokenize_amr_data(preprocessed_data, tokenizer):\n    \"\"\"\n    Tokenizes the preprocessed AMR data.\n\n    Args:\n    - preprocessed_data (list of dict): Each dict contains 'input' (serialized AMR) and 'output' (target text).\n    - tokenizer: Tokenizer instance from Hugging Face, like GPT2Tokenizer.\n\n    Returns:\n    - tokenized_data (list of dict): Each dict has tokenized 'input_ids' and 'labels' (for the output).\n    \"\"\"\n\n    tokenized_data = []\n\n    for entry in preprocessed_data:\n        # Tokenize the input and output separately\n        input_ids = tokenizer.encode(entry['input'], add_special_tokens=True)\n        labels = tokenizer.encode(entry['output'], return_tensors='pt', add_special_tokens=True).squeeze()\n        \n        # Store tokenized input and output\n        tokenized_data.append({\n            'input_ids': input_ids,\n            'labels': labels\n        })\n\n    return tokenized_data\n\n# Example usage\n# Initialize the tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n# Add any special tokens if necessary, e.g., \"<sos>\", \"<eos>\"\ntokenizer.add_special_tokens({'pad_token': '<pad>'})\n\n# Tokenize the preprocessed data\ntokenized_data = tokenize_amr_data(bfs_train_processed[:24], tokenizer)\nfor item in tokenized_data:\n    print(f\"Input IDs: {item['input_ids']}\")\n    print(f\"Labels: {item['labels']}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile run.py\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n# from datautils import MyTrainDataset\n\nimport torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nimport os\n\n\nfrom transformers import BartTokenizer, BartForConditionalGeneration, AdamW, get_scheduler, MBartTokenizer\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom tqdm import tqdm\n\n\ndef ddp_setup(rank, world_size):\n    \"\"\"\n    Args:\n        rank: Unique identifier of each process\n        world_size: Total number of processes\n    \"\"\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n    torch.cuda.set_device(rank)\n    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n\nclass QALD_9_AMRDataset(Dataset):\n    def __init__(self, dsetType: str, tokenizer, train):\n        with open(f'{dsetType}_train.json', 'r') as f:\n            train = json.load(f)\n        with open(f'{dsetType}_test.json', 'r') as f:\n            test = json.load(f) \n     \n        bfs_train_processed = preprocess_amr_data(train)\n        dfs_train_processed = preprocess_amr_data(train)\n\n\n        self.bfs_train = tokenizer(source_raw, truncation=True, padding='max_length', max_length=384, return_tensors = 'pt')\n        self.bfs_test = tokenizer(target_raw, truncation=True, padding='max_length', max_length=384, return_tensors = 'pt')\n        \n    def __len__(self): # returns the total number of samples in the dataset\n        assert self.source_tok['input_ids'].shape[0] == self.target_tok['input_ids'].shape[0]\n        return self.source_tok['input_ids'].shape[0]\n        \n    def __getitem__(self, idx):\n        return {'input_ids': self.source_tok['input_ids'][idx], \n                'attention_mask': self.source_tok['attention_mask'][idx],\n                'labels': self.target_tok['input_ids'][idx],            \n               }\n\nclass Trainer:\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        train_data: DataLoader,\n        val_data: DataLoader,\n        tokenizer: BartTokenizer,\n        optimizer: torch.optim.Optimizer,\n        gpu_id: int,\n        save_every: int,\n    ) -> None:\n        self.gpu_id = gpu_id\n        self.model = model.to(gpu_id)\n        self.train_data = train_data\n        self.val_data = val_data\n        self.tokenizer = tokenizer\n        self.optimizer = optimizer\n        self.save_every = save_every\n        self.model = DDP(model, device_ids=[gpu_id])\n\n    def _run_batch(self, input_ids, attention_mask, labels, run_type='train', eval_loss=None):\n        if run_type == 'train':\n            self.optimizer.zero_grad()\n            # print(input_ids)\n            # print(labels)\n            # print(input_ids.shape, attention_mask.shape, labels.shape)\n            output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            # print(output)\n            # loss = F.cross_entropy(output, targets)\n            loss = output.loss\n            # print(loss)\n            ### TODO: Add lr_scheduler ###\n            loss.backward()\n\n            self.optimizer.step()\n            return output, eval_loss\n        elif run_type == 'validate':\n            with torch.no_grad():\n                output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                eval_loss += output.loss.item()\n            return output, eval_loss\n\n    def _run_epoch(self, epoch, epoch_type='train'):\n\n        if epoch_type == 'train':\n            print(\"------ Training! ------\")\n            data = self.train_data\n            eval_loss = None\n\n        elif epoch_type == 'validate':\n            print(\"------ Validating! ------\")\n            data = self.val_data\n            eval_loss = 0\n\n        # b_sz = len(next(iter(self.train_data))[0])\n        # b_sz = len(next(iter(data))[0])\n        # print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Steps: {len(data)}\")\n        data.sampler.set_epoch(epoch)\n        # for source, targets in self.train_data:\n        #     source = source.to(self.gpu_id)\n        #     targets = targets.to(self.gpu_id)\n        #     self._run_batch(source, targets)\n        for batch_idx, batch in tqdm(enumerate(data), total=len(data)):\n            input_ids = batch['input_ids'].to(self.gpu_id)\n            attention_mask = batch['attention_mask'].to(self.gpu_id)\n            labels = batch['labels'].to(self.gpu_id)\n\n            if epoch_type == 'train':\n                _, __ = self._run_batch(input_ids=input_ids, attention_mask=attention_mask, labels=labels, eval_loss=eval_loss, run_type='train')\n            elif epoch_type == 'validate':\n                _, eval_loss = self._run_batch(input_ids=input_ids, attention_mask=attention_mask, labels=labels, eval_loss=eval_loss, run_type='validate')\n        if epoch_type == 'validate':\n            print(f\"Epoch {epoch+1}: Evaluation Loss = {eval_loss / len(self.val_data)}\")\n\n    def _save_checkpoint(self, epoch):\n        ckp = self.model.module.state_dict()\n        PATH = \".\"\n        #         torch.save(ckp, PATH)\n        self.model.module.save_pretrained(PATH)\n        self.tokenizer.save_pretrained(PATH)\n\n        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n\n    def train(self, max_epochs: int):\n        for epoch in range(max_epochs):\n            self._run_epoch(epoch=epoch, epoch_type='train')\n            self._run_epoch(epoch=epoch, epoch_type='validate')\n            if self.gpu_id == 0 and (epoch + 1) % self.save_every == 0:\n                self._save_checkpoint(epoch)\n                \n    def generate_predictions(self, texts):\n        model = self.model.module\n        model.to('cuda')\n        model.eval()\n        predictions = []\n    \n        for text in texts:\n            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n            with torch.no_grad():\n                output_sequences = model.generate(**inputs.to('cuda'), num_beams=3, max_length=300, pad_token_id=model.config.eos_token_id)\n        \n            decoded_preds = self.tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n            predictions.extend(decoded_preds)\n    \n        return predictions \n\ndef load_train_objs(preTmodel):\n    # train_set = MyTrainDataset(2048)  # load your dataset\n    #     model_path = './model_cache/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177'\n    if preTmodel == 'bart':\n        tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n        model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n    elif preTmodel == 'gpt2':\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS (since GPT-2 does not have a dedicated pad token)\n        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n    new_tokens = ['<H>', '<R>', '<T>']\n    tokenizer.add_special_tokens({'additional_special_tokens': new_tokens})\n    model.resize_token_embeddings(len(tokenizer))\n    dataset_path = './webnlg/'\n    train_dataset = QALD_9_AMRDataset('bfs', tokenizer)\n    val_dataset = QALD_9_AMRDataset('bfs', tokenizer)\n    # model = torch.nn.Linear(20, 1)  # load your model\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n    return train_dataset, val_dataset, model, tokenizer, optimizer\n\n\ndef prepare_dataloader(dataset: Dataset, batch_size: int):\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        pin_memory=True,\n        shuffle=False,\n        sampler=DistributedSampler(dataset)\n    )\n\ndef main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):\n    ddp_setup(rank, world_size)\n    train_dataset, val_dataset, model, tokenizer, optimizer = load_train_objs(\"gpt2\")\n    train_data = prepare_dataloader(train_dataset, batch_size)\n    val_data = prepare_dataloader(val_dataset, batch_size)\n    trainer = Trainer(model, train_data, val_data, tokenizer, optimizer, rank, save_every)\n    trainer.train(total_epochs)\n    with open('./webnlg/'+ 'test_both.source', 'r') as f:\n        test_source = f.readlines()\n    with open('./webnlg/' + 'test_both.target', 'r') as f:\n        test_target = f.readlines()\n    predictions = trainer.generate_predictions(test_source)\n    from nltk.translate.bleu_score import corpus_bleu\n    references = [[ref.split()] for ref in test_target]  # Reference texts should be a list of lists of tokens\n    predicted_tokens = [pred.split() for pred in predictions]  # Predictions should also be a list of lists of tokens\n    bleu_score = corpus_bleu(references, predicted_tokens)\n    print(f\"BLEU score: {bleu_score:.4f}\")\n    destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description='simple distributed training job')\n    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n    parser.add_argument('--batch_size', default=8, type=int, help='Input batch size on each device (default: 32)')\n    args = parser.parse_args()\n\n    world_size = torch.cuda.device_count()\n    mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T02:16:15.447158Z","iopub.execute_input":"2024-11-25T02:16:15.447525Z","iopub.status.idle":"2024-11-25T02:16:15.457203Z","shell.execute_reply.started":"2024-11-25T02:16:15.447493Z","shell.execute_reply":"2024-11-25T02:16:15.456442Z"}},"outputs":[{"name":"stdout","text":"Overwriting run.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!python run.py 6 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T02:16:21.140609Z","iopub.execute_input":"2024-11-25T02:16:21.141215Z","iopub.status.idle":"2024-11-25T02:16:39.130910Z","shell.execute_reply.started":"2024-11-25T02:16:21.141183Z","shell.execute_reply":"2024-11-25T02:16:39.130079Z"}},"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n[rank0]:[W1125 02:16:37.397867305 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW1125 02:16:37.674000 133721619326784 torch/multiprocessing/spawn.py:146] Terminating process 186 via signal SIGTERM\nTraceback (most recent call last):\n  File \"/kaggle/working/qald-amr-dataset/run.py\", line 220, in <module>\n    mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 282, in spawn\n    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 238, in start_processes\n    while not context.join():\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 189, in join\n    raise ProcessRaisedException(msg, error_index, failed_process.pid)\ntorch.multiprocessing.spawn.ProcessRaisedException: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 76, in _wrap\n    fn(i, *args)\n  File \"/kaggle/working/qald-amr-dataset/run.py\", line 193, in main\n    train_dataset, val_dataset, model, tokenizer, optimizer = load_train_objs(\"gpt2\")\n  File \"/kaggle/working/qald-amr-dataset/run.py\", line 175, in load_train_objs\n    train_dataset = QALD_9_AMRDataset(dataset_path, 'train', tokenizer)\n  File \"/kaggle/working/qald-amr-dataset/run.py\", line 32, in __init__\n    with open(f'{dsetType}_train.json', 'r') as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'train_train.json'\n\n","output_type":"stream"}],"execution_count":6}]}