# -*- coding: utf-8 -*-
"""GNN_to_Gpt2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14taqXwo8SxmD_zopeq27-ot37lMgCuSZ
"""

# !pip install torch_geometric penman

# from google.colab import files
# uploads=files.upload()
import torch
import torch
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import GCNConv, RGCNConv, GATConv
# import networkx as nx
from collections import defaultdict
from torch.utils.data import Dataset
import networkx as nx
import penman
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from torch_geometric.loader import DataLoader as GeoDataLoader


DEVICE=torch.device("cuda:2")


class TextGraphDataset(Dataset):
    def __init__(self, input_ids, attention_masks, graphs):
        assert len(input_ids) == len(graphs), "Text and graph data must have the same length"
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.graphs = graphs

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            "input_ids": self.input_ids[idx],
            "attention_mask": self.attention_masks[idx],
            "graph": self.graphs[idx]
        }

# Initialize the dataset

def make_amr_graph_lists(data_file):
    amr_graphs = list()
    with open(data_file,'r',encoding='utf-8') as f:
      amr_data_train=f.read()
    graphs=list(penman.loads(amr_data_train))
    text_list=list()
    # print(graphs.metadata.get('snt',None))
    # print(graphs[7])
    # for i in range(len(graphs)):
    for i,g  in enumerate(graphs):
      nx_graph = nx.DiGraph()
      # g=graphs[i]
    #   print(g)
      # print(g.metadata.get('snt',None))
      text_list.append(g.metadata.get('snt',None))
    # Add nodes and edges to the graph
      for source, relation, target in g.triples:
        # print(source, target)
          nx_graph.add_edge(source, target, label=relation)
      amr_graphs.append(nx_graph)
    #   print("text list",text_list)
    return amr_graphs,text_list


# Assume `amr_graphs` is a list of AMR graphs represented as NetworkX DiGraph objects
# Each graph should have labeled nodes and edges.

# Example list of AMR graphs (NetworkX DiGraph format)
amr_graphs,text_list = make_amr_graph_lists('qald-9-amr-train.txt')  # Replace with a list of NetworkX graphs


model_path='gpt2'
gpt2_model=GPT2LMHeadModel.from_pretrained("gpt2")
gpt2_tokenizer=GPT2Tokenizer.from_pretrained("gpt2")


def tokenize_text(text_list,tokenizer):
  tokenized_defn=tokenizer(text_list,padding=True,truncation=True,return_tensors='pt')
  input_ids=tokenized_defn['input_ids'].to(DEVICE)
  attn_mask=tokenized_defn['attention_mask'].to(DEVICE)
  return input_ids,attn_mask


def collate_fn(batch):
    # Extract text and graph data
    input_ids = torch.stack([item["input_ids"] for item in batch])
    attention_masks = torch.stack([item["attention_mask"] for item in batch])
    graphs = [item["graph"] for item in batch]

    # Batch graphs using PyTorch Geometric's DataLoader collate_fn
    batched_graphs = GeoDataLoader(graphs, batch_size=len(graphs)).collate_fn(graphs)

    return {
        "input_ids": input_ids,
        "attention_mask": attention_masks,
        "graphs": batched_graphs
    }

# Example function to convert each NetworkX AMR graph to PyTorch Geometric Data
def convert_amr_graph_to_data(graph, node_label_map, edge_label_map):
    # Node features (one-hot or embedding-based)
    node_features = []
    node_index_map = {}  # Map nodes to indices for edge indexing

    for i, node in enumerate(graph.nodes()):
        node_index_map[node] = i
        label = graph.nodes[node].get("label", "unknown")  # Assume nodes have "label" attribute
        node_features.append(torch.tensor(node_label_map[label], dtype=torch.float))

    # Stack node features into a single tensor
    x = torch.stack(node_features)  # Shape: [num_nodes, feature_dim]

    # Edge index and edge types
    edge_index = []
    edge_type = []
    for source, target, attr in graph.edges(data=True):
        edge_index.append((node_index_map[source], node_index_map[target]))
        label = attr.get("label", "relation")  # Assume edges have "label" attribute
        edge_type.append(edge_label_map[label])

    # Convert edge_index to tensor
    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()  # Shape: [2, num_edges]
    edge_type = torch.tensor(edge_type, dtype=torch.long)  # Shape: [num_edges]

    # Create PyTorch Geometric Data object
    data = Data(x=x, edge_index=edge_index, edge_type=edge_type)
    return data

# Define mappings for node and edge labels to unique indices
# print(amr_graphs[0].nodes.values())
unique_node_labels = set(node.get("label","unknown") for g in amr_graphs for node in g.nodes.values())
unique_edge_labels = set(edge.get("label", "relation") for g in amr_graphs for _, _, edge in g.edges(data=True))

# Create mappings
node_label_map = {label: idx for idx, label in enumerate(unique_node_labels)}
edge_label_map = {label: idx for idx, label in enumerate(unique_edge_labels)}

# Encode node labels as one-hot vectors
node_label_map = {label: torch.eye(len(node_label_map))[idx] for label, idx in node_label_map.items()}

# Convert all NetworkX graphs to PyTorch Geometric format
data_list = [convert_amr_graph_to_data(g, node_label_map, edge_label_map) for g in amr_graphs]
gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
gpt2_model.resize_token_embeddings(len(gpt2_tokenizer))
input_ids,attn_mask=tokenize_text(text_list,gpt2_tokenizer)
dataset=TextGraphDataset(input_ids,attn_mask,data_list)
# dataset.to(DEVICE)
# Create a DataLoader for batching
loader = DataLoader(dataset, batch_size=32,collate_fn=collate_fn, shuffle=True)

# for batch in loader:
#   print(batch['graph'])

import torch.nn as nn
import torch.nn.functional as F

class AttentionGNNEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, heads=1):
        super(AttentionGNNEncoder, self).__init__()
        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=0.1)
        self.gat2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=0.1)
        self.output_dim = output_dim

    def forward(self, x, edge_index):
        # First GAT layer with attention
        x = self.gat1(x, edge_index)
        x = torch.relu(x)
        
        # Second GAT layer
        x = self.gat2(x, edge_index)
        return x.mean(dim=0)  # Aggregate node embeddings into a single graph embedding

# import torch.nn as nn
class GNNtoGPT2(nn.Module):
    def __init__(self, gnn_encoder, gpt2_model, gpt2_hidden_dim):
        super(GNNtoGPT2, self).__init__()
        self.gnn_encoder = gnn_encoder
        self.gpt2_model = gpt2_model
        self.linear = nn.Linear(gnn_encoder.output_dim, gpt2_hidden_dim)  # Project GNN output to GPT-2's hidden size

    def forward(self, x, edge_index, input_ids, attention_mask=None):
        # Encode the graph with the GNN
        graph_embedding = self.gnn_encoder(x, edge_index)

        # Project graph embedding to GPT-2 hidden dimension
        gpt2_embedding = self.linear(graph_embedding).unsqueeze(0)  # Shape: [1, gpt2_hidden_dim]
        padding_mask = input_ids != gpt2_tokenizer.pad_token_id
        input_ids[~padding_mask] = 0  # Replace padding token IDs with 0


        # Prepare GPT-2 inputs
        inputs_embeds = self.gpt2_model.transformer.wte(input_ids)  # Token embeddings
        inputs_embeds[:, 0] = gpt2_embedding  # Replace the first token embedding with the GNN embedding

        # Decode with GPT-2
        outputs = self.gpt2_model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)
        return outputs

import torch.optim as optim
import torch.nn.functional as F
# Hyperparameters
input_dim = data_list[0].x.size(1)  # Input feature dimension
hidden_dim = 768
num_relations = len(edge_label_map)
learning_rate = 0.01
epochs = 100

# Instantiate model and optimizer
gnn_encoder = AttentionGNNEncoder(input_dim=input_dim, hidden_dim=hidden_dim,output_dim=768)
model=GNNtoGPT2(gnn_encoder,gpt2_model,768)
model.to(DEVICE)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()

# Training loop
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in loader:
        optimizer.zero_grad()
        # batch_graph=batch.batched_graphs
        batch_x=batch['graph'].x.to(DEVICE)
        batch_edge_index=batch['graph'].edge_index.to(DEVICE)
        # batch_edge_type=batch.edge_type
        batch_input_ids=batch['input_ids'].to(DEVICE)
        batch_attention_mask=batch['attention_mask'].to(DEVICE)
        # print(gpt2_tokenizer.pad_token_id)
        out = model(batch_x, batch_edge_index, batch_input_ids,batch_attention_mask)  # Forward pass
        logits = out.logits

        # Sample loss (modify for your task, e.g., node or graph classification)
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = batch_input_ids[..., 1:].contiguous()

    # Compute loss
        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        loss.backward()

        optimizer.step()

        total_loss += loss.item()
    print(f"Epoch {epoch + 1}, Loss: {total_loss / len(loader)}")